<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[测试]]></title>
    <url>%2F2020%2F02%2F29%2F%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[测试本篇文章属于测试篇章，主要查看系统是否正常。]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[神经网络算法]]></title>
    <url>%2F2019%2F09%2F01%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[神经网络关于感知机，既有好消息，也有坏清息，好消息是，即便对于复杂的函数，感知机也隐含眷能够表示它的可能性.上一章已经介绍过，即便是计算机进行的复杂处理，感知机（理论上）也可以将其表示出来。坏消息是，设定权重的工作，即确定合适的、能符合预期的输入与输出的权重，现在还是由人工进行的。 神经网络的出现就是为了解决刚才的坏消息。具体地讲，神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。本章中，我们会先介绍神经网络的概要，然后重点关注神经网络进行识别时的处理。 一般而言，“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数(阶跃函数是指一旦输入超过阈值，就切换输出的函数)的模型。“多层感知机”是指神经网络，即使用sigmoid函数等平滑的激活函数的多层网络。 激活函数激活函数是链接感知机和神经网络的桥梁。 sigmoid函数的平滑性对于神经网络的学习具有重要意义。 感知机中神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号。 阶跃函数和sigmoid函数，此两者均为非线性函数(指的是不像线性函数那样呈现出一条直线的函数)。 神经网络的激活函数必须使用非线性函数。 多维数组的运算数组的维度可以通过np.dim()函数获得，数组的形状可以通过实例变量shape获得，其返回的结果是一个元组(tuple)。 对于两个2*2的矩阵A和B，它们的乘积可以通过NumPy的 np.dot(A,B) 函数计算。np.dot()接收两个NumPy数组作为参数，并返回数组的乘积。不过，np.dot(A,B) 和 np.dot(B,A) 的值可能不一样。同样，其他形状的矩阵的乘积也可以使用相同的方法来计算。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法介绍]]></title>
    <url>%2F2019%2F09%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[前言谷歌董事长施密特曾说过：虽然谷歌的无人驾驶汽车和机器人受到了许多媒体关注，但是这家公司真正的未来在于机器学习，一种让计算机更聪明、更个性化的技术。 也许我们生活在人类历史上最关键的时期：从使用大型计算机，到个人电脑，再到现在的云计算。关键的不是过去发生了什么，而是将来会有什么发生。 工具和技术的民主化，让像我这样的人对这个时期兴奋不已。计算的蓬勃发展也是一样。如今，作为一名数据科学家，用复杂的算法建立数据处理机器一小时能赚到好几美金。但能做到这个程度可并不简单！我也曾有过无数黑暗的日日夜夜。 谁能从这篇指南里受益最多？我今天所给出的，也许是我这辈子写下的最有价值的指南。 这篇指南的目的，是为那些有追求的数据科学家和机器学习狂热者们，简化学习旅途。这篇指南会让你动手解决机器学习的问题，并从实践中获得真知。我提供的是几个机器学习算法的高水平理解，以及运行这些算法的 R 和 Python 代码。这些应该足以让你亲自试一试了。 我特地跳过了这些技术背后的数据，因为一开始你并不需要理解这些。如果你想从数据层面上理解这些算法，你应该去别处找找。但如果你想要在开始一个机器学习项目之前做些准备，你会喜欢这篇文章的。 广义来说，有三种机器学习算法1、 监督式学习工作机制：这个算法由一个目标变量或结果变量（或因变量）组成。这些变量由已知的一系列预示变量（自变量）预测而来。利用这一系列变量，我们生成一个将输入值映射到期望输出值的函数。这个训练过程会一直持续，直到模型在训练数据上获得期望的精确度。监督式学习的例子有：回归、决策树、随机森林、K – 近邻算法、逻辑回归等。 2、非监督式学习工作机制：在这个算法中，没有任何目标变量或结果变量要预测或估计。这个算法用在不同的组内聚类分析。这种分析方式被广泛地用来细分客户，根据干预的方式分为不同的用户组。非监督式学习的例子有：关联算法和 K – 均值算法。 3、强化学习工作机制：这个算法训练机器进行决策。它是这样工作的：机器被放在一个能让它通过反复试错来训练自己的环境中。机器从过去的经验中进行学习，并且尝试利用了解最透彻的知识作出精确的商业判断。 强化学习的例子有马尔可夫决策过程。 常见机器学习算法名单这里是一个常用的机器学习算法名单。这些算法几乎可以用在所有的数据问题上： 线性回归逻辑回归决策树SVM朴素贝叶斯K最近邻算法K均值算法随机森林算法降维算法Gradient Boost 和 Adaboost 算法1、线性回归线性回归通常用于根据连续变量估计实际数值（房价、呼叫次数、总销售额等）。我们通过拟合最佳直线来建立自变量和因变量的关系。这条最佳直线叫做回归线，并且用 Y= a *X + b 这条线性等式来表示。 理解线性回归的最好办法是回顾一下童年。假设在不问对方体重的情况下，让一个五年级的孩子按体重从轻到重的顺序对班上的同学排序，你觉得这个孩子会怎么做？他（她）很可能会目测人们的身高和体型，综合这些可见的参数来排列他们。这是现实生活中使用线性回归的例子。实际上，这个孩子发现了身高和体型与体重有一定的关系，这个关系看起来很像上面的等式。 在这个等式中： Y：因变量a：斜率x：自变量b ：截距系数 a 和 b 可以通过最小二乘法获得。 参见下例。我们找出最佳拟合直线 y=0.2811x+13.9 。已知人的身高，我们可以通过这条等式求出体重。 线性回归的两种主要类型是一元线性回归和多元线性回归。一元线性回归的特点是只有一个自变量。多元线性回归的特点正如其名，存在多个自变量。找最佳拟合直线的时候，你可以拟合到多项或者曲线回归。这些就被叫做多项或曲线回归。 Python 代码 #Import Library #Import other necessary libraries like pandas, numpy…from sklearn import linear_model #Load Train and Test datasets #Identify feature and response variable(s) and values must be numeric and numpy arraysx_train=input_variables_values_training_datasetsy_train=target_variables_values_training_datasetsx_test=input_variables_values_test_datasets Create linear regression objectlinear = linear_model.LinearRegression() Train the model using the training sets and check scorelinear.fit(x_train, y_train)linear.score(x_train, y_train) #Equation coefficient and Interceptprint(‘Coefficient: n’, linear.coef_)print(‘Intercept: n’, linear.intercept_) #Predict Outputpredicted= linear.predict(x_test)R代码 #Load Train and Test datasets #Identify feature and response variable(s) and values must be numeric and numpy arraysx_train &lt;- input_variables_values_training_datasetsy_train &lt;- target_variables_values_training_datasetsx_test &lt;- input_variables_values_test_datasetsx &lt;- cbind(x_train,y_train) Train the model using the training sets and check scorelinear &lt;- lm(y_train ~ ., data = x)summary(linear) #Predict Outputpredicted= predict(linear,x_test)2、逻辑回归别被它的名字迷惑了！这是一个分类算法而不是一个回归算法。该算法可根据已知的一系列因变量估计离散数值（比方说二进制数值 0 或 1 ，是或否，真或假）。简单来说，它通过将数据拟合进一个逻辑函数来预估一个事件出现的概率。因此，它也被叫做逻辑回归。因为它预估的是概率，所以它的输出值大小在 0 和 1 之间（正如所预计的一样）。 让我们再次通过一个简单的例子来理解这个算法。 假设你的朋友让你解开一个谜题。这只会有两个结果：你解开了或是你没有解开。想象你要解答很多道题来找出你所擅长的主题。这个研究的结果就会像是这样：假设题目是一道十年级的三角函数题，你有 70%的可能会解开这道题。然而，若题目是个五年级的历史题，你只有30%的可能性回答正确。这就是逻辑回归能提供给你的信息。 从数学上看，在结果中，几率的对数使用的是预测变量的线性组合模型。 odds= p/ (1-p) = probability of event occurrence / probability of not event occurrenceln(odds) = ln(p/(1-p))logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3….+bkXk在上面的式子里，p 是我们感兴趣的特征出现的概率。它选用使观察样本值的可能性最大化的值作为参数，而不是通过计算误差平方和的最小值（就如一般的回归分析用到的一样）。 现在你也许要问了，为什么我们要求出对数呢？简而言之，这种方法是复制一个阶梯函数的最佳方法之一。我本可以更详细地讲述，但那就违背本篇指南的主旨了。 Python代码 12345678910111213141516#Import Libraryfrom sklearn.linear_model import LogisticRegression#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create logistic regression objectmodel = LogisticRegression()# Train the model using the training sets and check scoremodel.fit(X, y)model.score(X, y)#Equation coefficient and Interceptprint('Coefficient: n', model.coef_)print('Intercept: n', model.intercept_)#Predict Outputpredicted= model.predict(x_test) R代码 x &lt;- cbind(x_train,y_train) Train the model using the training sets and check scorelogistic &lt;- glm(y_train ~ ., data = x,family=’binomial’)summary(logistic) #Predict Outputpredicted= predict(logistic,x_test)更进一步：你可以尝试更多的方法来改进这个模型： 加入交互项精简模型特性使用正则化方法使用非线性模型3、决策树这是我最喜爱也是最频繁使用的算法之一。这个监督式学习算法通常被用于分类问题。令人惊奇的是，它同时适用于分类变量和连续因变量。在这个算法中，我们将总体分成两个或更多的同类群。这是根据最重要的属性或者自变量来分成尽可能不同的组别。想要知道更多，可以阅读：简化决策树 。 来源： statsexchange 在上图中你可以看到，根据多种属性，人群被分成了不同的四个小组，来判断 “他们会不会去玩”。为了把总体分成不同组别，需要用到许多技术，比如说 Gini、Information Gain、Chi-square、entropy。 理解决策树工作机制的最好方式是玩Jezzball，一个微软的经典游戏（见下图）。这个游戏的最终目的，是在一个可以移动墙壁的房间里，通过造墙来分割出没有小球的、尽量大的空间。 因此，每一次你用墙壁来分隔房间时，都是在尝试着在同一间房里创建两个不同的总体。相似地，决策树也在把总体尽量分割到不同的组里去。 更多信息请见： 决策树算法的简化 Python代码 #Import Library #Import other necessary libraries like pandas, numpy…from sklearn import tree #Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset Create tree objectmodel = tree.DecisionTreeClassifier(criterion=’gini’) for classification, here you can change the algorithm as gini or entropy (information gain) by default it is ginimodel = tree.DecisionTreeRegressor() for regressionTrain the model using the training sets and check scoremodel.fit(X, y)model.score(X, y) #Predict Outputpredicted= model.predict(x_test)R代码 library(rpart)x &lt;- cbind(x_train,y_train) grow treefit &lt;- rpart(y_train ~ ., data = x,method=”class”)summary(fit) #Predict Outputpredicted= predict(fit,x_test)4、支持向量机这是一种分类方法。在这个算法中，我们将每个数据在N维空间中用点标出（N是你所有的特征总数），每个特征的值是一个坐标的值。 举个例子，如果我们只有身高和头发长度两个特征，我们会在二维空间中标出这两个变量，每个点有两个坐标（这些坐标叫做支持向量）。 现在，我们会找到将两组不同数据分开的一条直线。两个分组中距离最近的两个点到这条线的距离同时最优化。 上面示例中的黑线将数据分类优化成两个小组，两组中距离最近的点（图中A、B点）到达黑线的距离满足最优条件。这条直线就是我们的分割线。接下来，测试数据落到直线的哪一边，我们就将它分到哪一类去。 更多请见： 支持向量机的简化 将这个算法想作是在一个 N 维空间玩 JezzBall。需要对游戏做一些小变动： 比起之前只能在水平方向或者竖直方向画直线，现在你可以在任意角度画线或平面。游戏的目的变成把不同颜色的球分割在不同的空间里。球的位置不会改变。Python代码 #Import Libraryfrom sklearn import svm #Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset Create SVM classification objectmodel = svm.svc() there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.Train the model using the training sets and check scoremodel.fit(X, y)model.score(X, y) #Predict Outputpredicted= model.predict(x_test)R代码 library(e1071)x &lt;- cbind(x_train,y_train) Fitting modelfit &lt;-svm(y_train ~ ., data = x)summary(fit) #Predict Outputpredicted= predict(fit,x_test)5、朴素贝叶斯在预示变量间相互独立的前提下，根据 贝叶斯定理 可以得到朴素贝叶斯这个分类方法。用更简单的话来说，一个朴素贝叶斯分类器假设一个分类的特性与该分类的其它特性不相关。举个例子，如果一个水果又圆又红 ， 并且直径大约是 3 英寸，那么这个水果可能会是苹果。即便这些特性互相依赖 ， 或者依赖于别的特性的存在，朴素贝叶斯分类器还是会假设这些特性分别独立地暗示这个水果是个苹果。 朴素贝叶斯模型易于建造，且对于大型数据集非常有用。虽然简单，但是朴素贝叶斯的表现却超越了非常复杂的分类方法。 贝叶斯定理提供了一种从P(c)、P(x)和P(x|c) 计算后验概率 P(c|x) 的方法。请看以下等式： 在这里， P ( c|x ) 是已知预示变量（属性）的前提下，类（目标）的后验概率P ( c ) 是类的先验概率P ( x|c ) 是可能性，即已知类的前提下，预示变量的概率P ( x ) 是预示变量的先验概率例子：让我们用一个例子来理解这个概念。在下面，我有一个天气的训练集和对应的目标变量“Play”。现在，我们需要根据天气情况，将会“玩”和“不玩”的参与者进行分类。让我们执行以下步骤。 步骤1：把数据集转换成频率表。 步骤2：利用类似“当Overcast可能性为0.29时，玩耍的可能性为0.64”这样的概率，创造 Likelihood 表格。 步骤3：现在，使用朴素贝叶斯等式来计算每一类的后验概率。后验概率最大的类就是预测的结果。 问题：如果天气晴朗，参与者就能玩耍。这个陈述正确吗？ 我们可以使用讨论过的方法解决这个问题。于是 P（会玩 | 晴朗）= P（晴朗 | 会玩）* P（会玩）/ P （晴朗） 我们有 P （晴朗 |会玩）= 3/9 = 0.33，P（晴朗） = 5/14 = 0.36, P（会玩）= 9/14 = 0.64 现在，P(会玩 | 晴朗）= 0.33 * 0.64 / 0.36 = 0.60，有更大的概率。 朴素贝叶斯使用了一个相似的方法，通过不同属性来预测不同类别的概率。这个算法通常被用于文本分类，以及涉及到多个类的问题。 Python代码 #Import Libraryfrom sklearn.naive_bayes import GaussianNB #Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer linkTrain the model using the training sets and check scoremodel.fit(X, y) #Predict Outputpredicted= model.predict(x_test)R代码 library(e1071)x &lt;- cbind(x_train,y_train) Fitting modelfit &lt;-naiveBayes(y_train ~ ., data = x)summary(fit) #Predict Outputpredicted= predict(fit,x_test)6、KNN（K – 最近邻算法）该算法可用于分类问题和回归问题。然而，在业界内，K – 最近邻算法更常用于分类问题。K – 最近邻算法是一个简单的算法。它储存所有的案例，通过周围k个案例中的大多数情况划分新的案例。根据一个距离函数，新案例会被分配到它的 K 个近邻中最普遍的类别中去。 这些距离函数可以是欧式距离、曼哈顿距离、明式距离或者是汉明距离。前三个距离函数用于连续函数，第四个函数（汉明函数）则被用于分类变量。如果 K=1，新案例就直接被分到离其最近的案例所属的类别中。有时候，使用 KNN 建模时，选择 K 的取值是一个挑战。 更多信息：K – 最近邻算法入门（简化版） 我们可以很容易地在现实生活中应用到 KNN。如果想要了解一个完全陌生的人，你也许想要去找他的好朋友们或者他的圈子来获得他的信息。 在选择使用 KNN 之前，你需要考虑的事情： KNN 的计算成本很高。变量应该先标准化（normalized），不然会被更高范围的变量偏倚。在使用KNN之前，要在野值去除和噪音去除等前期处理多花功夫。Python代码 #Import Libraryfrom sklearn.neighbors import KNeighborsClassifier #Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset Create KNeighbors classifier object modelKNeighborsClassifier(n_neighbors=6) default value for n_neighbors is 5Train the model using the training sets and check scoremodel.fit(X, y) #Predict Outputpredicted= model.predict(x_test)R代码 library(knn)x &lt;- cbind(x_train,y_train) Fitting modelfit &lt;-knn(y_train ~ ., data = x,k=5)summary(fit) #Predict Outputpredicted= predict(fit,x_test)7、K 均值算法K – 均值算法是一种非监督式学习算法，它能解决聚类问题。使用 K – 均值算法来将一个数据归入一定数量的集群（假设有 k 个集群）的过程是简单的。一个集群内的数据点是均匀齐次的，并且异于别的集群。 还记得从墨水渍里找出形状的活动吗？K – 均值算法在某方面类似于这个活动。观察形状，并延伸想象来找出到底有多少种集群或者总体。 K – 均值算法怎样形成集群： K – 均值算法给每个集群选择k个点。这些点称作为质心。每一个数据点与距离最近的质心形成一个集群，也就是 k 个集群。根据现有的类别成员，找出每个类别的质心。现在我们有了新质心。当我们有新质心后，重复步骤 2 和步骤 3。找到距离每个数据点最近的质心，并与新的k集群联系起来。重复这个过程，直到数据都收敛了，也就是当质心不再改变。如何决定 K 值： K – 均值算法涉及到集群，每个集群有自己的质心。一个集群内的质心和各数据点之间距离的平方和形成了这个集群的平方值之和。同时，当所有集群的平方值之和加起来的时候，就组成了集群方案的平方值之和。 我们知道，当集群的数量增加时，K值会持续下降。但是，如果你将结果用图表来表示，你会看到距离的平方总和快速减少。到某个值 k 之后，减少的速度就大大下降了。在此，我们可以找到集群数量的最优值。 Python代码 #Import Libraryfrom sklearn.cluster import KMeans #Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset Create KNeighbors classifier object modelk_means = KMeans(n_clusters=3, random_state=0) Train the model using the training sets and check scoremodel.fit(X) #Predict Outputpredicted= model.predict(x_test)R代码 library(cluster)fit &lt;- kmeans(X, 3) # 5 cluster solution8、随机森林随机森林是表示决策树总体的一个专有名词。在随机森林算法中，我们有一系列的决策树（因此又名“森林”）。为了根据一个新对象的属性将其分类，每一个决策树有一个分类，称之为这个决策树“投票”给该分类。这个森林选择获得森林里（在所有树中）获得票数最多的分类。 每棵树是像这样种植养成的： 如果训练集的案例数是 N，则从 N 个案例中用重置抽样法随机抽取样本。这个样本将作为“养育”树的训练集。假如有 M 个输入变量，则定义一个数字 m&lt;&lt;M。m 表示，从 M 中随机选中 m 个变量，这 m 个变量中最好的切分会被用来切分该节点。在种植森林的过程中，m 的值保持不变。尽可能大地种植每一棵树，全程不剪枝。若想了解这个算法的更多细节，比较决策树以及优化模型参数，我建议你阅读以下文章： 随机森林入门—简化版将 CART 模型与随机森林比较（上）将随机森林与 CART 模型比较（下）调整你的随机森林模型参数Python #Import Libraryfrom sklearn.ensemble import RandomForestClassifier #Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset Create Random Forest objectmodel= RandomForestClassifier() Train the model using the training sets and check scoremodel.fit(X, y) #Predict Outputpredicted= model.predict(x_test)R代码 library(randomForest)x &lt;- cbind(x_train,y_train) Fitting modelfit &lt;- randomForest(Species ~ ., x,ntree=500)summary(fit) #Predict Outputpredicted= predict(fit,x_test)9、降维算法在过去的 4 到 5 年里，在每一个可能的阶段，信息捕捉都呈指数增长。公司、政府机构、研究组织在应对着新资源以外，还捕捉详尽的信息。 举个例子：电子商务公司更详细地捕捉关于顾客的资料：个人信息、网络浏览记录、他们的喜恶、购买记录、反馈以及别的许多信息，比你身边的杂货店售货员更加关注你。 作为一个数据科学家，我们提供的数据包含许多特点。这听起来给建立一个经得起考研的模型提供了很好材料，但有一个挑战：如何从 1000 或者 2000 里分辨出最重要的变量呢？在这种情况下，降维算法和别的一些算法（比如决策树、随机森林、PCA、因子分析）帮助我们根据相关矩阵，缺失的值的比例和别的要素来找出这些重要变量。 想要知道更多关于该算法的信息，可以阅读 《降维算法的初学者指南》 。 Python代码 #Import Libraryfrom sklearn import decomposition #Assumed you have training and test data set as train and test Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features)For Factor analysis#fa= decomposition.FactorAnalysis() Reduced the dimension of training dataset using PCAtrain_reduced = pca.fit_transform(train) #Reduced the dimension of test datasettest_reduced = pca.transform(test) #For more detail on this, please refer this link.R Code library(stats)pca &lt;- princomp(train, cor = TRUE)train_reduced &lt;- predict(pca,train)test_reduced &lt;- predict(pca,test)10、Gradient Boosting 和 AdaBoost 算法当我们要处理很多数据来做一个有高预测能力的预测时，我们会用到 GBM 和 AdaBoost 这两种 boosting 算法。boosting 算法是一种集成学习算法。它结合了建立在多个基础估计值基础上的预测结果，来增进单个估计值的可靠程度。这些 boosting 算法通常在数据科学比赛如 Kaggl、AV Hackathon、CrowdAnalytix 中很有效。 更多： 详尽了解 Gradient 和 AdaBoost Python代码 #Import Libraryfrom sklearn.ensemble import GradientBoostingClassifier #Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset Create Gradient Boosting Classifier objectmodel= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0) Train the model using the training sets and check scoremodel.fit(X, y) #Predict Outputpredicted= model.predict(x_test)R代码 library(caret)x &lt;- cbind(x_train,y_train) Fitting modelfitControl &lt;- trainControl( method = “repeatedcv”, number = 4, repeats = 4)fit &lt;- train(y ~ ., data = x, method = “gbm”, trControl = fitControl,verbose = FALSE)predicted= predict(fit,x_test,type= “prob”)[,2]GradientBoostingClassifier 和随机森林是两种不同的 boosting 树分类器。人们常常问起这两个算法之间的区别。 结语现在我能确定，你对常用的机器学习算法应该有了大致的了解。写这篇文章并提供 Python 和 R 语言代码的唯一目的，就是让你立马开始学习。如果你想要掌握机器学习，那就立刻开始吧。做做练习，理性地认识整个过程，应用这些代码，并感受乐趣吧！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库、数据表的基本操作及查询数据]]></title>
    <url>%2F2019%2F09%2F01%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E8%A1%A8%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%8F%8A%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[数据库的基本操作 创建数据库 CREATE DATABASE database_name database_name为要创建的数据库的名称 删除数据库 DROP DATABASE database_name database_name为要删除的数据库的名称 数据库存储引擎数据库存储引擎是数据库底层软件组成，数据库管理系统(DBMS)使用数据引擎进行创建、查询、更新和删除数据操作。MySQL的核心就是存储引擎。 存储引擎比较 功能 MYISAM MEMORY INNODB ARCHIVE 存储限制 256TB RAM 64TB None 支持事务 No No Yes No 支持全文索引 Yes No No No 支持数索引 Yes Yes Yes No 支持哈希索引 No Yes No No 支持数据缓存 No N/A Yes No 支持外键 No No Yes No 数据表的基本操作创建数据表 创建表的语法形式 1234567CREATE TABLE&lt;表名&gt;( 字段名1 数据类型 [列级别约束条件] [默认值], 字段名2 数据类型 [列级别约束条件] [默认值], …… [表级别约束条件]); 使用主键约束 主键约束要求主键列的数据唯一，并且不允许为空。他能唯一地标识表中的一条记录，可以结合外键来定义不同数据表之间的关系，并且可以加快数据库查询的速度。 单字段主键 在定义列的同时指定主键。 字段名 数据类型 PRIMARY KEY [默认值] 在定义完所有列之后指定主键。 [CONSTRAINT &lt;约束名&gt;] PRIMARY KEY [字段名] 多字段联合主键主键由多个字段联合组合而成。 PRIMARY KEY [字段1、字段2...]其位置放置在定义完所有的主键之后 使用外键约束 外键用来在两个表的数据之间建立链接，它可以是一列或者多列。一个表可以有一个或多个外键。 [CONSTRAINT&lt;外键名&gt;] FOREIGN KEY 字段1[,字段2...] REFERENCES&lt;主表名&gt; 主键列1[,主键列2...] 使用非空约束 非空约束指字段的值不能为空。对于使用了非空约束的字段，如果用户在添加数据时没有指定值，数据库系统会报错。 字段名 数据类型 NOT NULL 使用唯一性约束 唯一性约束要求该列唯一，允许为空，但只能出现一个空值。 唯一约束可以确保一列或者几列不出现重复值。 在定义完列之后直接指定唯一约束。字段名 数据类型 UNIQUE 在定义完所有列之后指定唯一约束。[CONSTRAINT&lt;约束名&gt;] UNIQUE(&lt;字段名&gt;) 使用默认约束 默认约束指定某列的默认值。 字段名 数据类型 DEFAULT 默认值 设置表的属性值自动增加 在数据库应用中，可以通过为表的主键添加 AUTO_INCREMENT关键字来实现：当每新增加一条记录，使该主键自动加一。一个表只能有一个字段使用 AUTO_INCREMENT约束，且该字段必须为主键的一部分。其约束的字段可以是任何整数类型。 字段名 数据类型 AUTO_INCREMENT 查看数据表结构 查看表基本结构语句DESCRIBE 表名; 或者简写为 DESC 表名; 查看表详细结构语句SHOW CREATE TABLE &lt;表名/G&gt;;如果不加 /G参数，显示的结果可能非常混乱，加上参数 /G后，可使显示结果更加直观，易于查看。 修改数据表 修改表名ALTER TABLE &lt;旧表名&gt; RENAME [TO] &lt;新表名&gt;; 修改字段的数据类型ALTER TABLE &lt;表名&gt; MODIFY &lt;字段名&gt; &lt;数据类型&gt;; 修改字段名ALTER TABLE &lt;表名&gt; CHANGE &lt;旧字段名&gt; &lt;新字段名&gt; &lt;新数据类型&gt;; 添加字段ALTER TABLE &lt;表名&gt; ADD &lt;新字段名&gt; &lt;新数据类型&gt; [约束条件] [FIRST|AFTER 已存在字段名]; 删除字段ALTER TABLE &lt;表名&gt; DROP &lt;字段名&gt;; 修改字段的排列位置ALTER TABLE &lt;表名&gt; MODIFY &lt;字段1&gt; &lt;数据类型&gt; FIRST|AFTER &lt;字段2&gt;; 更改表的存储引擎ALTER TABLE &lt;表名&gt; ENGINE=&lt;更改后的存储引擎名&gt;; 删除表的外键约束ALTER TABLE &lt;表名&gt; DROP FOREIGN KEY &lt;外键约束名&gt;; 删除数据表DROP TABLE [IF EXISTS] 表1,表2...表n; 查询数据基本查询语句1234567891011121314SELECT &#123;*|&lt;字段列表&gt;&#125; [ FROM &lt;表1&gt;,&lt;表2&gt;... [WHERE &lt;表达式&gt;] [GROUP BY &lt;group by definition&gt;] [HAVING &lt;expression&gt;[&#123;&lt;operator&gt;&lt;expression&gt;&#125;...]] [ORDER BY &lt;order by definition&gt;] [LIMIT [offset,] &lt;row count&gt;] ] SELECT [字段1,字段2,...,字段n] FROM [表或视图] WHERE [查询条件]; 单表查询 查询所有字段SELECT * FROM 表名; 查询指定字段SELECT 字段名1[,字段名2,...,字段名n] FROM 表名; 查询指定记录在 SELECT语句中，通过 WHERE子句可以对数据进行过滤。SELECT 字段名1[,字段名2,...,字段名n] FROM 表名 WHERE 查询条件; 带 IN关键字的查询IN操作符用来查询满足指定范围内的条件的记录，使用 IN操作符，将所有检索条件用括号括起来，检索条件之间用逗号隔开，只要满足条件范围内的一个值即为匹配项。在 IN关键字前面加上 NOT即可使得查询的结果正好相反。 带 BETWEEN AND的范围查询BETWEEN AND用来查询某个范围内的值，该操作符需要有两个参数，即范围的开始值和结束。如果字段值满足指定的范围查询条件，则这些记录被返回。同样，在BETWEEN AND关键字前面加上 NOT即可使得查询的结果正好相反。 带 LIKE的字符匹配查询LIKE关键字即是使用通配符来进行匹配查找。通配符是一种在SQL的 WHERE条件子句中拥有特殊意思的字符，可以和 LIKE一起使用的通配符有 %和 _。 百分号通配符 %，匹配任意长度的字符，甚至包括零字符。下划线通配符 _ ，一次只能匹配任意一个字符。 查询空值 空值不同于0，也不同于空字符串。空值一般表示数据未知、不适用或将在以后添加数据。 在 SELECT语句中使用 IS NULL子句，可以查询某字段内容为空记录。 带 AND的多条件查询AND主要用于 WHERE子句中，用来链接两个甚至多个查询条件，表示所有的条件都需要满足才会返回值。 带 OR的多条件查询OR也主要用于 WHERE子句中，用来链接两个甚至多个查询条件，表示所有的条件仅需满足其中之一项便会返回值。 查询结果不重复 在 SELECT语句中，使用 DISTINCT关键字来指示MySQL消除重复的记录。 SELECT DISTINCT 字段名 FROM 表名; 对查询结果排序 用 ORDER BY语句来对查询的结果进行排序。 在后面添加 DESC表示降序排序 在后面添加 ASC或默认，表示升序排序 分组查询在MySQL中使用 `GROUP BY`来对数据进行分组 `[GROUP BY 字段] [HAVING &lt;条件表达式&gt;] [WITH ROLLUP]` `HAVING` 关键字用来过滤数据，因为 `WHERE`不能和 `GROUP BY`混用 `WITH ROLLUP`关键字是在所有查询出的记录之后增加一条记录，该记录计算查询出的所有记录的总和，即统计记录数量。但是`ROLLUP`和能够与 `GROUP BY`同时使用的`ORDER BY`不能同时使用。 使用 LIMIT限制查询结果的数量 LIMIT [位置偏移量,] 行数 使用集合函数查询 函数 作用 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值的和 连接查询 内连接查询在内连接查询中，只有满足条件的记录才能出现在结果关系中。两个表之间的关系通过 (INNER) JOIN指定。 使用这种语法的时候，连接的条件使用 ON子句给出，而不是用 WHERE。 外连接查询 LEFT JOIN左连接 返回包括左表中的所有记录和右表中连接字段相等的记录。 RIGHT JOIN右连接 返回包括右表中的所有记录和左表中连接字段相等的记录。 复合条件连接查询 复合条件连接查询是在连接查询的过程中，通过添加过滤条件，限制查询的结果，使查询的结果更加准确。 子查询 子查询指一个查询语句嵌套在另一个查询语句内部的查询。 带 ANY、SOME关键字的子查询ANY和 SOME关键字是同义词，表示满足其中任一条件，它们允许创建一个表达式对子查询的返回值列表进行比较，只要满足内层子查询中的任何一个比较条件，就返回一个结果作为外层查询的条件。 带 ALL关键字的子查询使用 ALL时，需要同时满足所有内层查询的条件。 带 EXISTS关键字的子查询EXISTS关键字后面的参数是一个任一的子查询，系统对子查询进行运算以判断它是否返回行，如果至少返回一行，那么 EXISTS的结果为 true，此时外层查询语句将进行查询；如果子查询没有返回任何行，那么 EXISTS的结果为 false，此时外层语句不进行查询。 带 IN关键字的子查询使用 IN关键字进行子查询时，内层查询语句仅仅返回一个数据列，这个数据列里的值将提供给外层查询语句进行比较操作。当外层查询语句内只要有一个内层查询语句返回的数据列中的数据时，则判断为满足条件，外层查询语句将进行查询。 带比较运算符的子查询子查询可以使用如 &#39;&lt;&#39;,&#39;&lt;=&#39;,&#39;=&#39;,&#39;&gt;&#39;,&#39;&gt;=&#39;,&#39;!=&#39;等比较运算符。 合并查询结果利用 UNION关键字，可以给出多条 SELECT语句，并将他们的结果组合成单个结果集。合并时，两个表对应的列数和数据类型必须相同。各个 SELECT语句之间使用 UNION或 UNION ALL关键字分割。 UNION不适用关键字 ALL，执行的时候删除重复的记录，所有返回的行都是唯一的；使用关键字 ALL的作用时不删除重复行也不对结果进行自动排序。 为表和字段取别名 为表取别名表名 [AS] 表别名 为字段取别名列名 [AS] 列别名 有时为了方便，也会把AS省略，之间在后面写上别名，和前面内容用空格隔开就好 使用正则表达式查询 MYSQL中使用 REGEXP关键字指定正则表达式的字符匹配模式。 正则表达式常用字符匹配列表|选项|说明|| :———–: | —————— ||^|匹配文本的开始字符||$|匹配文本的结束字符||.|匹配任何单子符||*|匹配零个或多个在他前面的字符||+ |匹配前面的字符1次或多次||&lt;字符串&gt;|匹配保护指定的字符串的文本||[字符集合]|匹配字符集合中的任何一个字符||[^]|匹配不在括号中的任何字符||字符串{n,}|匹配前面的字符串至少n次||字符串{n,m}|匹配前面的字符串至少n次，至多m次|]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>MYSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何成为数据科学家]]></title>
    <url>%2F2019%2F09%2F01%2F%E6%88%90%E4%B8%BA%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%2F</url>
    <content type="text"><![CDATA[成为数据科学家！ 事实上，你可以成为一名真正的数据科学家，且不需要掌握这些技能。NoSQL和MapReduce不是新概念————在这些关键词被创建之前，就有很多人接触到它们。但要成为一名数据科学家，你需要以下能力。 敏锐的商业头脑。 真正的大数据专业知识（例如，可以在几个小时内快速地处理一个5000万行的数据集）。 认知数据的能力。 对模型具有猜凝精神。 了解大数据”诅咒”。 有能力沟通并理解管理人员正在试图解决哪些问题。 能正确评估付你工资所能带来的回报（ROI）或效益提升（lift）。 能够快速地识别一个简单的、健壮的、可扩展性的解决方案。 能够说服推动管理人员，即使不情愿，也要为了公司、用户和股东的利益，转到正确的方向上。 真正热爱数据分析。 成功案例的实际应用经验。 数据架构知识。 数据收集和清理技能。 计算复杂度的基础知识一如何开发健壮的、商效的、可扩展的、可移植的架构。 良好的算法知识。 数据科学家在商业分析、统计学和计算机科学等领域也是通才，比如会掌握这些专业知识:健壮性、实验设计、算法复杂度、仪表盘和数据可视化。一些数据科家也是数据策略师————他们可以开发数据收集策略，并使用数据来发现可操作的、能对商业产生影响的见解。这就要求数据科学家具有创造性，能根据业务要求，分析、提出解决方案。 要理解数据科学，所需的基本数学知识包括: 代数，如果可能的话，包括基本矩阵理论。 微积分入门课程。要掌握的理论不多，只需要理解计算的复杂度和o标记法即可。了解特殊函数，包括对数、指数、暴蹈数。微分方程、积分和复数不是必要的。 统计与概览的入门课程，要了解随机变量、概率、均值、方差、百分位数、实验设计、交叉验证、拟合度和稳健统计的概念。 从技术的角度，要掌握的重要技能和知识有R、Python、Excel、SQL、图形（可视化）、FTP基本的UNIX命令（sort、grep、head、tail、管道和重定向操作符、cat、cron定时等），以及对如何设计和访问数据库有基本了解。了解分布式系统如何工作和在哪里能发现瓶颈（是在硬盘和内存之间的数据传输，还是在互联网上），这也很重要。最后，要了解网络爬虫基本知识，有助于获取互联网上能找到的非结构化数据。]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[感知机算法]]></title>
    <url>%2F2019%2F09%2F01%2F%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[感知机算法感知机接受多个信号，输出一个信号。这里所说的“信号”可以想象成电流或河流那样具备“流动性”的东西。像电流流过导线，向前方输送电子一样，感知机的信号也会形成流，向前方输送信息。但是，和实际的电流不同的是，感知机的信号只有“流/不流”(1/0)两种取值。 比如，x1、x2是输入信号，y是输出信号，w1、w2是权重(w是weight的首字母)。输入信号被送往神经元时，会被分别乘以固定的权重(w1x1、w2x2)。神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出1。这也成为“神经元被激活”。这里将这个界限值成为阈值，用符号θ 表示。 小结： 感知机是具有输入和输出的算法，给定一个输入后，将输出一个既定的值。 感知机将权重和偏置设定为参数。 使用感知机可以表示与门和或门等逻辑电路。 异或门无法通过单层感知机来表示。 使用2层感知机可以表示异或门。 单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。 多层感知机(在理论上)可以表示计算机。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[你的眼里有星辰和大海]]></title>
    <url>%2F2019%2F09%2F01%2F%E4%BD%A0%E7%9A%84%E7%9C%BC%E9%87%8C%E6%9C%89%E6%98%9F%E8%BE%B0%E5%92%8C%E5%A4%A7%E6%B5%B7%2F</url>
    <content type="text"><![CDATA[你的眼里有星辰和大海，胜过我看过的所有春花秋月 最近自从开始找工作后，却越发觉得自己对于未来的道路很迷茫。不知该去向何方，也不知去找一个怎样的工作。 通过在BOSS直聘和前程无忧两家平台上面投递自己的简历，倒也接到过几家面试邀请。不过更多的，却是失败的消息。 面试的第一家公司，浙江森马电子商务有限公司 在整个的面试过程当中，谈的倒是挺愉快的。内容也大致就是较为详细的询问了下最近做的项目，以及周边的一些情况。之后也大致谈了一下针对后期培训过程中的事情。总之，三个面试官(部门HR，部门技术主管，部门经理)倒也没感到有什么压力存在，很像和许久不见的朋友在那聊天的感觉，也挺轻松愉快的。 不过，可能是因为自身的一些原因多一些吧。比如，技术主管问我对于那个项目具体用到的算法提出了质疑，而我却因当时突然大脑麻木了一下，就没有很准确的回答上来。也有可能是别的原因。比如，HR告诉我说更期望招聘一个在电商行业有相关经验的人来做，那样会更合适一些。也有可能是他们发现了比我更合适的人选的缘故。 总之，也是各方面原因综合造成的吧，很不幸没有面试成功。 面试的第二家公司， 杭州湃沃电子商务有限公司 这个好像是我第一次经历二轮面试，结果被刷下来的公司。 第一轮面试，HR先互相大概了解了下具体的情况，便说需要等待通知。及其幸运的是，当天晚上的时候，HR小姐姐就来电话了，让我准备明天上午的复试。 第二轮面试，就直接是技术面了。这边竟然也是小姐姐当技术主管，好神奇。不过简单聊了聊之后我便发现，这份工作可能不是我想要的那样。 更期待找一份能够在专一领域内做一名既懂业务需求，也了解技术框架的那种综合型的大数据分析师。当任务需求到来的时候，我能够独挡一面，完全可以把这一件事情很完美的做出来，而且做出来的效果也很好。 只不过，这个领域，我真的不知道该去哪里追寻。是电商吗？是物流吗？还是金融？亦或是教育行业？更甚于其他行业？这么多的选择，真的是够头疼的。可是我却在此迷失了方向。 不过好在我知道了自己的不足， 而这恰恰是湃沃电商的技术主管小姐姐让我深思的问题。因为当听她说道，电商行业的数据分析，大致就是联系运营和仓库的一个桥梁。通过分析对比历史的数据，来考虑哪些货物分别属于爆平滞，哪些货物需要搞促销来完成企业整体的业绩。其实内容倒也很繁琐无味，真的谈不上什么高深技术可言。听完这些后，我觉得电商行业已经不适合我了，太没有挑战性了。 我的眼里有星辰和大海，心中有勇敢和深情。虽然未来尚未到来，但我已做好充足准备。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[win10系统永久激活方法]]></title>
    <url>%2F2019%2F09%2F01%2Fwin10%E7%B3%BB%E7%BB%9F%E6%B0%B8%E4%B9%85%E6%BF%80%E6%B4%BB%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[使用win10系统专业版的新用户注意了，没有激活win10系统的话，就会经常出现系统运行迟钝的现象，如果没有激活win10专业版，用起来很难操作，怎么使得win10专业版永久激活呢，我们知道win10专业版永久激活的步骤就行了。 需要激活的网友在网上找到了一些激活码，但是激活之后只有短短的一段的时间，又会失效，怎么才能够找到永久激活的办法使得win10专业版永久激活呢？小编这就跟大家说说win10专业版永久激活的方法。 win10专业版永久激活 按组合按钮“win+r”打开“运行对话框”，输入命令“slmgr.vbs -xpr”，点击确定 发现系统未激活状态。 在“此电脑”鼠标右键，打开属性。 在开始菜单上，点击鼠标右键，选择命令提示符{管理员} 输入命令 slmgr /ipk VK7JG-NPHTM-C97JM-9MPGT-3V66T slmgr /skms kms.xspace.in slmgr /ato 然后，按回车. 操作完以上步骤后，按win+r打开运行，输入命令slmgr.vbs -xpr，点击确定，进行查看。 激活win10系统成功了。 以上就是win10系统永久激活的一些操作步骤。 附录win10激活码如下， 单语言版：BT79Q-G7N6G-PGBYW-4YWX6-6F4BT 专业版：W269N-WFGWX-YVC9B-4J6C9-T83GX，VK7JG-NPHTM-C97JM-9MPGT-3V66T 企业版：NPPR9-FWDCX-D2C8J-H872K-2YT43，TX9XD-98N7V-6WMQ6-BX7FG-H8Q99 家庭版：TX9XD-98N7V-6WMQ6-BX7FG-H8Q99，XGVPP-NMH47-7TTHJ-W3FW7-8HV2C 教育版：NW6C2-QMPVW-D7KKK-3GKT6-VCFB2 专业版N：MH37W-N47XK-V7XM9-C7227-GCQG9 企业版N：DPH2V-TTNVB-4X9Q3-TJR4H-KHJW4 教育版N：2WH4N-8QGBV-H22JP-CT43Q-MDWWJ 企业版LSTB：WNMTR-4C88C-JK8YV-HQ7T2-76DF9 企业版LSTB N：2F77B-TNFGY-69QQF-B8YKP-D69TJ]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python之NumPy详解(一)]]></title>
    <url>%2F2019%2F09%2F01%2FPython%E4%B9%8BNumPy%E8%AF%A6%E8%A7%A3(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[NumPy简介NumPy(Numerical Python) 是 Python 语言的一个扩展程序库，支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。NumPy 的前身 Numeric 最早是由 Jim Hugunin 与其它协作者共同开发，2005 年，Travis Oliphant 在 Numeric 中结合了另一个同性质的程序库 Numarray 的特色，并加入了其它扩展而开发了 NumPy。NumPy 为开放源代码并且由许多协作者共同维护开发。NumPy 是一个运行速度非常快的数学库，主要用于数组计算，包含： 一个强大的N维数组对象 ndarray 广播功能函数 整合 C/C++/Fortran 代码的工具 线性代数、傅里叶变换、随机数生成等功能 NumPy应用NumPy 通常与 SciPy（Scientific Python）和 Matplotlib（绘图库）一起使用， 这种组合广泛用于替代 MatLab，是一个强大的科学计算环境，有助于我们通过 Python 学习数据科学或者机器学习。SciPy 是一个开源的 Python 算法库和数学工具包。SciPy 包含的模块有最优化、线性代数、积分、插值、特殊函数、快速傅里叶变换、信号处理和图像处理、常微分方程求解和其他科学与工程中常用的计算。Matplotlib 是 Python 编程语言及其数值数学扩展包 NumPy 的可视化操作界面。它为利用通用的图形用户界面工具包，如 Tkinter, wxPython, Qt 或 GTK+ 向应用程序嵌入式绘图提供了应用程序接口（API）。 相关连接 NumPy 官网 NumPy 源代码 SciPy 官网 SciPy 源代码 Matplotlib 官网 Matplotlib 源代码 NumPy Ndarray 对象NumPy 最重要的一个特点是其 N 维数组对象 ndarray，它是一系列同类型数据的集合，以 0 下标为开始进行集合中元素的索引。ndarray 对象是用于存放同类型元素的多维数组。ndarray 中的每个元素在内存中都有相同存储大小的区域。ndarray 内部由以下内容组成： 一个指向数据（内存或内存映射文件中的一块数据）的指针。 数据类型或 dtype，描述在数组中的固定大小值的格子。 一个表示数组形状（shape）的元组，表示各维度大小的元组。 一个跨度元组（stride），其中的整数指的是为了前进到当前维度下一个元素需要”跨过”的字节数。 ndarray 的内部结构: 跨度可以是负数，这样会使数组在内存中后向移动，切片中 obj[::-1] 或 obj[:,::-1] 就是如此。创建一个 ndarray 只需调用 NumPy 的 array 函数即可：numpy.array(object, dtype = None, copy = True, order = None, subok = False, ndmin = 0)参数说明： 名称 描述 object 数组或嵌套的数列 dtype 数组元素的数据类型，可选 copy 对象是否需要复制，可选 order 创建数组的样式，C为行方向，F为列方向，A为任意方向（默认） subok 默认返回一个与基类类型一致的数组 ndmin 指定生成数组的最小维度 NumPy 数据类型numpy 支持的数据类型比 Python 内置的类型要多很多，基本上可以和 C 语言的数据类型对应上，其中部分类型对应为 Python 内置的类型。下表列举了常用 NumPy 基本类型。 名称 描述 bool_ 布尔型数据类型（True 或者 False） int_ 默认的整数类型（类似于 C 语言中的 long，int32 或 int64） intc 与 C 的 int 类型一样，一般是 int32 或 int 64 intp 用于索引的整数类型（类似于 C 的 ssize_t，一般情况下仍然是 int32 或 int64） int8 字节（-128 to 127） int16 整数（-32768 to 32767） int32 整数（-2147483648 to 2147483647） int64 整数（-9223372036854775808 to 9223372036854775807） uint8 无符号整数（0 to 255） uint16 无符号整数（0 to 65535） uint32 无符号整数（0 to 4294967295） uint64 无符号整数（0 to 18446744073709551615） float_ float64 类型的简写 float16 半精度浮点数，包括：1 个符号位，5 个指数位，10 个尾数位 float32 单精度浮点数，包括：1 个符号位，8 个指数位，23 个尾数位 float64 双精度浮点数，包括：1 个符号位，11 个指数位，52 个尾数位 complex_ complex128 类型的简写，即 128 位复数 complex64 复数，表示双 32 位浮点数（实数部分和虚数部分） complex128 复数，表示双 64 位浮点数（实数部分和虚数部分） numpy 的数值类型实际上是 dtype 对象的实例，并对应唯一的字符，包括 np.bool_，np.int32，np.float32，等等。 数据类型对象 (dtype)数据类型对象是用来描述与数组对应的内存区域如何使用，这依赖如下几个方面： 数据的类型（整数，浮点数或者 Python 对象） 数据的大小（例如， 整数使用多少个字节存储） 数据的字节顺序（小端法或大端法） 在结构化类型的情况下，字段的名称、每个字段的数据类型和每个字段所取的内存块的部分 如果数据类型是子数组，它的形状和数据类型 字节顺序是通过对数据类型预先设定”&lt;”或”&gt;”来决定的。”&lt;”意味着小端法(最小值存储在最小的地址，即低位组放在最前面)。”&gt;”意味着大端法(最重要的字节存储在最小的地址，即高位组放在最前面)。 dtype 对象是使用以下语法构造的：numpy.dtype(object, align, copy) object - 要转换为的数据类型对象 align - 如果为 true，填充字段使其类似 C 的结构体。 copy - 复制 dtype 对象 ，如果为 false，则是对内置数据类型对象的引用]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NumPy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础知识]]></title>
    <url>%2F2019%2F09%2F01%2FPython%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Python应用场景 数据分析 数据采集 人工智能 编程开发 工具脚本 Python基本变量类型 类型 描述 语法示例 整型(int) 无小数部分的数 1,100 浮点型(float) 有小数部分的数 1.2,100.5 字符串(string) 不可变的字符序列 ‘1’,’我很好’ 布尔型 True、False两种值 空值空值是Python里一个特殊的值，用None表示。None不能理解为0，因为0是有意义的，而None是一个特殊的空值。 常见的数据类型转换 函数 说明 int(x) 将x转换成一个整数 float(x) 将x转换成一个浮点数 str(x) 将对象x转换成字符串 repr(x) 将对象x转换成表达式字符串 eval(str) 用来计算在字符串中的有效Python表达式，并返回一个对象 tuple(s) 将序列s转换为一个元祖 list(s) 将序列s转换为一个列表 命名规则 最重要的一点就是要见名知意可以使用小驼峰式、大驼峰式，或者用下划线”_”来连接所有的单词 Python的格式化输出在Python中，采用的格式化方式和C语言是一致的，用%实现。在字符串内部，%s表示用字符串替换，%d表示用整数替换，有几个%?占位符，后面就跟几个变量或者值，顺序要对应好。如果只有一个%?，括号可以省略。|格式控制|输出|解释||-|-|-||”%d”|24|格式为整数||”%5d”|24|24，宽度为5个字符||”%05d”|00024|宽度为5个字符，其余用0填充||”%f”|1.21997|浮点数||”%.2f”|1.22|小数点两位的浮点数||”%7.2f”|1.22|宽度为七，小数点两位的浮点数||”%s”|Hello|字符串||”%d%%”|24%|整数和百分号，%%输出一个百分号|]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python的逻辑判断和循环]]></title>
    <url>%2F2019%2F09%2F01%2FPython%E7%9A%84%E9%80%BB%E8%BE%91%E5%88%A4%E6%96%AD%E5%92%8C%E5%BE%AA%E7%8E%AF%2F</url>
    <content type="text"><![CDATA[逻辑判断if elif else 条件判断if语句的完整形式是： 123456if &lt;条件判断1&gt;: &lt;执行1&gt;elif &lt;条件判断2&gt;: &lt;执行2&gt;else: &lt;执行3&gt; if语句可以进行嵌套来实现多重语句的判断 循环结构 while循环 for循环break和continue 在循环中，break语句可以提前退出循环体continue语句可以跳过当前循环，直接开始下一次循环]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统常用指令]]></title>
    <url>%2F2019%2F09%2F01%2FLinux%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[这里介绍Linux系统里面较为常用的一些指令。 目录切换命令 123456cd usr 切换到该目录下的usr目录cd ../ 切换到上一层目录cd / 切换到系统根目录cd ~ 切换到用户主目录cd - 切换到上一个所在的目录pwd 显示当前目录 目录的操作命令 文件的操作命令 压缩文件的操作命令 其他命令 Linux的权限命令]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA“输出”命令的区别]]></title>
    <url>%2F2019%2F09%2F01%2FJAVA%E2%80%9C%E8%BE%93%E5%87%BA%E2%80%9D%E5%91%BD%E4%BB%A4%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[JAVA里面常用的控制台输出语句有 System.out.println 和System.out.print 一、两者的区别如下： 参数有区别 System.out.println() 可以不写参数 System.out.print(参数) 参数不能为空，并且必须有 效果有区别 println： 会在输出完信息后进行换行，产生一个新行 print：不会产生新行 println 更简洁 ， print 更灵活 print 可以在后面跟”\n”来达到和 println 一样的效果 print 也可以跟 “\t” 制表符，等 二、通过阅读java源代码来理解 12345678public static void main (String [] args)&#123; int x = 10 ; while (x &lt; 20 )&#123; System.out.println (x); x++ ; System.out.print ("\n";) &#125;&#125; 三、注意事项 虽然有略微的区别，但是两个方法经常一起搭配使用，更加灵活和方便。]]></content>
      <tags>
        <tag>JAVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F09%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[CentOS6.5mini版hadoop集群服务搭建流程]]></title>
    <url>%2F2019%2F09%2F01%2Fhadoop%E9%9B%86%E7%BE%A4%E6%9C%8D%E5%8A%A1%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[三个机器都需要执行的操作1.需要在win系统下安装VMware虚拟机2.需要在VMware虚拟机里面创建三台CentOS6.5 mini版的Linux系统3.将三台虚拟机的主机名分别设置为master和slave1与slave2通过修改network文件中的hostname文件来进行修改 1vi /etc/sysconfig/network 修改完成之后刷新下网络配置 1source /etc/sysconfig/network 如果不生效的话，就重启下机器 4.将其网络配置成 自动连接网络，并且需要是IPV4手动连接的方式5. 验证他们的网络配置是否正确通过使用 ifconfig 命令来进行查看想对应的ip地址是否正确 并且可以通过使用 1ping www.baidu.com 来查看其是否能连接到外部网络。 如果有错误的话，可以通过使用 1vi /etc/sysconfig/network-scripts/ifcfg-eth0 命令来进行修改里面的配置文件。 将NM_CONTROLLED修改为no 将BOOTPROTO修改为static 更改IPADDR的物理地址为一个新的地址 将HWADDR这一行命令删除掉 这里面存放了之前的MAC地址 6.重启网络服务1service network restart 7.删掉原来系统生成的网络规则1rm -rf /etc/udev/rules.d/70-persistent-net.rules 8.重启Linux系统，使三台电脑都能够连接到网络1reboot //这是重启电脑的命令行 9.安装ssh服务12yum -y install openssh-server 安装ssh服务器yum -y install openssh-clients 安装ssh客户端 10.关闭防火墙1service iptables stop 11.禁止防火墙开机启动1chkconfig iptables off 12.配置 /etc/hosts 文件在hosts文件中增加以下内容 1234192.168.37.101 master192.168.37.102 slave1192.168.37.103 slave2// 上面的IP地址为之前配置过的信息，而后面信息为三台虚拟机的hostname名字 13.生成机器的公钥和私钥1执行 ssh-keygen 命令来生成公钥和私钥 14. 把公钥发送给master1执行 ssh-copy-id master /root/.ssh/id_rsa.pub 来把公钥发送给master 接下来是master的操作1.检查是否收集到了所有的公钥1执行 cat /root/.ssh/authorized_keys 检查是否收集到了所有的公钥 2. 将公钥发送给slave1和slave2123// 通过执行下面的代码来把公钥发送给slave1和slave2scp /root/.ssh/authorized_keys root@slave1:/root/.ssh/scp /root/.ssh/authorized_keys root@slave2:/root/.ssh/ 三个机器都需要执行的操作分别登录到所有的机器，共九次123456ssh master //登录到masterexit //退出ssh slave1 //登录到slave1exit //退出ssh slave2 //登录到slave2exit //退出 接下来是master的操作1.创建一个存放软件的目录123//在 master系统内的opt 文件夹下 创建一个名为 SoftWare 的文件夹cd /opt/mkdir SoftWare 2.安装lrzsz软件安装可进行本机和虚拟机之间文件传输的软件 lrzsz 1yum -y install lrzsz 3.上传jdk到创建好SoftWare文件夹下用 rz 命令可以上传本地的文件到所在的目录 4.解压jdk文件1tar -xvf jdk-8u151-linux-x64.tar.gz 5. 配置jdk的环境变量1234567vi /etc/profile//通过编辑profile文件来配置环境变量//在文件的最后一行追加以下内容export JAVA_HOME=/opt/SoftWare/jdk1.8.0_151export JRE_HOME=/opt/SoftWare/jdk1.8.0_151/jreexport CLASSPATH=.:$JRE_HOME/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin 6.刷新环境变量1source /etc/profile 7.验证环境变量是否配置成功1java -version 如果出现java的版本信息，则证明java的环境变量已经配置成功了，否则还需要重新配置 8.上传Hadoop到创建好SoftWare文件夹下用 rz 命令可以上传本地的文件到所在的目录 9.解压Hadoop文件10.进入解压后的Hadoop文件夹创建五个新的文件夹创建名字为 tmp，logs，hdfs，hdfs/name，hdfs/data 的五个文件夹 11.使用nodepad++进行配置文件的修改 首先进入到 hadoop-2.7.3/etc/hadoop 文件夹下面 hadoop-env.sh 修改第 25 行的 ${JAVA_HOME} 为自己的 jdk 安装目录 ​ （/opt/SoftWare/jdk1.8.0_151） yarn-env.sh 修改第 23 行，解注释（export JAVA_HOME=/home/y/libexec/jdk1.6.0/），修改路径为自己的 jdk 安装目录 ​ （/opt/SoftWare/jdk1.8.0_151 ） slaves 修改 localhost 为 slave1 和 slave2 重命名 mapred-site.xml.template 通过指令的方式来进行重命名。源文件叫.template 修改为去掉.template mv mapred-site.xml.template mapred-site.xml 修改配置文件 配置 etc/hadoop/core-site.xml 123456789101112131415&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;!--定义HadoopMaster的URI和端口--&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;!--hadoop 中的临时存储目录，tmp 文件夹的路径 --&gt; &lt;value&gt;file:/opt/SoftWare/hadoop-2.7.3/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;!--用作序列化文件处理时读写buffer的大小--&gt; &lt;value&gt;131702&lt;/value&gt; &lt;/property&gt; 配置 etc/hadoop/hdfs-site.xml 123456789101112131415161718192021222324252627282930&lt;property&gt; &lt;!-- namenode 节点数据存储目录 --&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/opt/SoftWare/hadoop-2.7.3/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;!-- datanode 数据存储目录 --&gt; &lt;value&gt;file:/opt/SoftWare/hadoop-2.7.3/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定DataNode存储block的副本数量,不大于DataNode的个数就行 --&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定master的http地址 --&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定master的https地址 --&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;master:50091&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 必须设置为true，否则就不能通过web访问hdfs上的文件信息 --&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 配置 etc/hadoop/yarn-site.xml 123456789101112131415161718192021222324252627282930313233343536373839&lt;property&gt; &lt;!--NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序--&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--ResourceManager 对客户端暴露的地址。客户端通过该地址向RM提交应用程序，杀死应用程序等。--&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--ResourceManager 对ApplicationMaster暴露的访问地址。ApplicationMaster通过该地址向RM申请资源、释放资源等。--&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--ResourceManager 对NodeManager暴露的地址.。NodeManager通过该地址向RM汇报心跳，领取任务等。--&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--ResourceManager 对管理员暴露的访问地址。管理员通过该地址向RM发送管理命令等。--&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--用户可通过该地址在浏览器中查看集群各类信息。--&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--NodeManager总的可用物理内存。注意，该参数是不可修改的，一旦设置，整个运行过程中不 可动态修改。另外，该参数的默认值是8192MB，因此，这个值通过一 定要配置。不过，Apache已经正在尝试将该参数做成可动态修改的。--&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; 配置 etc/hadoop/mapred-site.xml 123456789101112&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt; 12.把 SoftWare文件夹拷贝到 slave1 和 slave2 的 opt 下12scp -r /opt/SoftWare/ root@slave1:/opt/scp -r /opt/SoftWare/ root@slave2:/opt/ 13.把 /etc/profile 文件拷贝到 slave1 和 slave2 的 /etc 下12scp /etc/profile/ root@slave1:/etc/scp /etc/profile/ root@slave2:/etc/ 三台虚拟机都需要执行的操作1.刷新环境变量，验证java是否安装成功12source /etc/profilejava -version 2.安装时间同步软件 ntpdate1yum -y install ntp ntpdate 3.和网络时间进行同步1ntpdate cn.pool.ntp.org 4.把时间写入硬件进行锁定1hwclock --systohc 接下来是master需要执行的操作1.进入到 hadoop-2.7.3/bin 目录2.格式化 hdfs使用 ./hdfs namenode -format 进行格式化 出现 /hdfs/name has been successfully formatted 表示成功 不允许多次格式化，否则会导致集群无法启动 如果出错的话： 1. 修改配置文件 2. 删除master机器上面的 hdfs/name 和hdfs/data 文件夹，并重新创建两个新的文件夹 3. 把修改的内容发送到 slave1 和slave2 下面，要时刻保持三个机器上面的配置文件是一直的 4. 重新格式化 master机器3.启动hdfs服务进入 hadoop-2.7.3/sbin 目录，使用 ./start-dfs.sh 启动 hdfs 服务 4.启动yarn服务进入 hadoop-2.7.3/sbin 目录，使用 ./start-yarn.sh 启动 yarn 服务 *启动全部服务如果所有的配置都没有发生问题的话，也可以在任意一个位置 使用 start-all.sh 命令来启动全部服务 三台虚拟机都需要执行的命令分别执行 jps 命令 来查看机器当前运行的服务如果 master 机器上面，有 NameNode SecondaryNameNode ResourceManager 这三个服务 如果slave 机器上面，有DataNode NodeManager 这两个服务 则证明我们的Hadoop集群已经搭建完成了 最终验证首先查看当前 master机器上面的IP地址为多少。例如： 192.168.37.101 通过浏览器访问 http://192.168.37.101:8088 查看当前Hadoop集群内部的运行状况 通过浏览器访问 http://192.168.37.101:50070 查看当前Hadoop集群上面的文件和整体集群的信息概览]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel之VBA语法基础]]></title>
    <url>%2F2019%2F09%2F01%2FExcel%E4%B9%8BVBA%E8%AF%AD%E6%B3%95%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[常量与变量 变量用于保存在策划稿女婿运行过程中需要临时保存的值或对象。变量具有不同的类型，也可能包含不同的数值。 在程序运行时，变量的数值可以改变。而当需要存储静态信息时，可以使用常量。 常量在VBA中，常量可以分为普通常量和符号常量两种。 普通常量普通常量大致有四种，分别是 数值常量 字符串常量 逻辑常量 日期常量符号常量符号常量是指用一个符号名来代替数值或字符串，又称为声明常量。其中，符号名必须是以字母开头，由字母、数字、下划线组成的长度不大于40的字符串，其格式为： 1Const 符号变量名 [As 类型] = 表达式[,符号变量名 = 表达式 ···] VBA的内置常量]]></content>
      <categories>
        <category>Excel</category>
      </categories>
      <tags>
        <tag>VBA</tag>
      </tags>
  </entry>
</search>
